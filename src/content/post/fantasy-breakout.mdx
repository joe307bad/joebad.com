---
title: Predicting breakout sleepers in fantasy football
category: analytics
publishedAt: 2025-09-21
subTitle: Combining fine-tuned composite scoring with machine learning to spot tomorrow's NFL fantasy breakout performance
tags:
  - machine learning
---

## Undervalued players in fantasy football

Building predictive models for fantasy football presents unique challenges. The game involves countless variables ranging from player health, team dynamics, and weather conditions, to coaching decisions and defensive matchups. This complexity, combined with the inherent volatility of single-game performances, makes accurate prediction particularly difficult. My approach to fantasy football has reflected this observation: I typically enable auto draft, minimize trades and transactions. I usaully adopt the role of watchmaker: creating my team and setting it in motion, intervening only when necessary.

Yet despite these challenges, patterns do emerge. Players with certain characteristics like favorable matchups, increasing snap counts, or specific team situations, tend to outperform expectations more often than pure chance would suggest. Participants who can identify these patterns and understand current NFL dynamics gain meaningful advantages.

More specifically, I am fascinated by finding undervalued players. I believe there are always players sitting on waivers who would be excellent replacements for injured or underperforming roster players. My hypothesis: every week, there are 3-5 available players who will outperform the lowest performer on your roster by at least 5 points. Of those possible 5 players, there is probably 1 who has a high potential for have a 10+ breakout performance that deserved a starting roster spot. The challenge is identifying them before they have a breakout performance. Let's test this hypothesis.

This blog post explores building a hybrid predictive model that combines domain expertise with machine learning. I developed a composite scoring system based on fantasy football fundamentals, then used that score as a key feature in a machine learning model. Using a dataset from the 2024 NFL season focusing on second-year player breakout predictions, I'll show how this hybrid approach improved predictive accuracy over using composite scoring alone.

## The Sleeper Score: Quantifying Breakout Potential

Before diving into machine learning, we need a solid foundation built on fantasy football fundamentals. The Sleeper Score is a composite metric designed to identify players with the highest probability of breaking out. It combines six weighted factors that fantasy experts have long recognized as predictive of future success, now quantified into a systematic 175-point scale.

### The Six Components of Sleeper Score

**1. Draft Capital (0-50 points) - The Opportunity Indicator**

Draft position reveals how NFL teams value a player's talent. The inverse weighting - where undrafted free agents score highest - reflects our focus on finding undervalued players. Players drafted early have already proven themselves; we're hunting for late-round picks and UDFAs who will exceed expectations.

- Undrafted Free Agent (UDFA): 50 points
- Draft pick > 200: 40 points
- Draft pick > 150: 30 points
- Draft pick > 100: 20 points
- Draft pick > 50: 10 points
- Draft pick ≤ 50: 0 points

**2. Performance History (0-30 points) - The Upside Gap**

Players with minimal Year 1 production have nowhere to go but up. This component rewards players who showed limited fantasy output in their rookie season, as they represent the largest potential improvement opportunity.

- Low production (&lt; 6 PPG for RB/WR, &lt; 4 PPG for TE): 30 points
- Minimal fantasy points: 20 points
- Moderate production (1-1.5x threshold): 20 points
- Higher production: 0 points

**3. Age Factor (0-20 points) - The Youth Advantage**

Younger players typically show steeper development curves. Players 22 and under are in prime physical development years and have more career runway for breakout performances.

- Age ≤ 22: 20 points
- Age ≤ 23: 15 points
- Age ≤ 24: 10 points
- Age > 24: 0 points

**4. Expert Consensus Ranking (0-20 points) - The Market Inefficiency**

FantasyPros ECR reveals where the fantasy community values players. Higher scores go to players with worse rankings (higher ECR numbers), indicating market undervaluation. This identifies the "wisdom of crowds" inefficiencies.

$$
\text{ECR Score} = 20 - \left(\frac{\text{ecr} - \text{min\_ecr}}{\text{max\_ecr} - \text{min\_ecr}}\right) \times 20
$$

**5. Defensive Matchup (0-30 points) - The Weekly Catalyst**

Matchups matter enormously in single-week performance. This position-specific component evaluates whether a player faces a weak defense in their relevant category (rush defense for RBs, pass defense for WRs/TEs).

- Opponent ranked 29-32 (worst defenses): 30 points
- Opponent ranked 25-28: 25 points
- Opponent ranked 21-24: 20 points
- Opponent ranked 17-20: 15 points
- Opponent ranked 13-16: 10 points
- Opponent ranked 9-12: 5 points
- Opponent ranked 1-8 (best defenses): 0 points

**6. Snap Count Trends (0-15 points) - The Momentum Signal**

Increasing playing time is the strongest leading indicator of fantasy production. This uses a 2-week sliding window to capture recent snap percentage momentum while filtering out single-week anomalies.

- Average delta ≥10%: 15 points (major increase)
- Average delta ≥5%: 12 points (significant increase)
- Average delta ≥2%: 8 points (moderate increase)
- Average delta >0%: 5 points (small increase)
- Average delta ≥-2%: 2 points (stable)
- Average delta &lt;-2%: 0 points (declining)

### Why This Rubric Identifies Breakout Candidates

The Sleeper Score succeeds because it captures the **convergence of opportunity and talent** that drives fantasy breakouts:

**Opportunity Identification:** The draft capital, snap trends, and matchup components identify when players are getting their chance. A UDFA with increasing snaps facing a bottom-tier defense represents maximum opportunity.

**Talent Validation:** While draft position matters, the age and ECR components ensure we're focusing on players the NFL and fantasy community still view as having upside. These aren't washed-up veterans—they're young players whose talent hasn't yet translated to fantasy production.

**Performance Gap:** The inverse relationship with Year 1 production is crucial. Players who barely played or produced poorly have massive room for improvement. A player averaging 2 PPG can realistically jump to 12+ PPG with increased opportunity. A player already at 10 PPG would need to reach elite status (20+ PPG) for similar improvement—far less likely for a waiver wire candidate.

**Weekly Volatility Capture:** Unlike seasonal projections, this rubric evaluates week-by-week factors (matchups, snap trends) that drive single-game explosions. Fantasy managers need players who can win a specific week, not necessarily maintain season-long consistency.

**Market Inefficiency Exploitation:** By focusing on players ranked outside the top 100 in ECR, we're targeting the space where fantasy managers aren't paying attention. These players sit on waivers, available for free, despite having quantifiable breakout indicators.

The 175-point maximum scale provides granular differentiation between candidates, allowing the machine learning model to learn which factor combinations most reliably predict actual breakouts. As we'll see in the analysis below, this composite score serves as the foundation for our hybrid ML approach, providing domain expertise that pure statistical models would struggle to discover independently.

## Analyzing 2024 for breakout performances

We analyzed 263 second-year player performances across 16 weeks of the 2024 NFL season (weeks 3-18), focusing on running backs, wide receivers, and tight ends. Each week, we predicted the top 10 most likely breakout candidates (players exceeding 5+ point fantasy improvement).

We consider these second-year players "sleepers" because they have an [Expert Consensus Rating](https://www.fantasypros.com/about/faq/football-draft-accuracy-methodology/) (which is basically average fantasy draft position) of greater than 100. So in a league of 12 teams, most rosters would be full before these players are drafted.

To build our predictive system, we start by building a composite scoring model using six weighted factors: draft capital, performance history, age, expert consensus rankings, defensive matchups, and snap count trends. Across 160 top-10 predictions (10 per week × 16 weeks), this baseline model correctly identified 39 breakout performances, achieving 24.4% accuracy.

We then developed a confidence-based model that weighted our composite score (60%) with binary playing time indicators (20%) and performance momentum metrics (20%). This hybrid approach improved to 51 correct predictions from 160 opportunities - a 31.9% accuracy rate and 31% relative improvement over the baseline. 

The hybrid approach demonstrates meaningful improvements in prediction accuracy and consistency across the 2024 NFL season, making it more reliable for fantasy football decision-making.

The data tells a clear story of improvement:

| Metric | Baseline Composite | Hybrid ML Model | Improvement |
|--------|-------------------|-----------------|-------------|
| **Weekly Accuracy** | 24.4% | 31.9% | +7.5 pts |
| **Total Correct Predictions** | 39 hits | 51 hits | +31% |
| **False Positive Rate** | Lower precision | Higher precision | Varies by week |
| **Best Week Performance** | 6 hits (Week 4) | 8 hits (Week 3, 4) | +33% |
| **Consistency** | More volatile | More stable | Better reliability |

**Weekly Performance Highlights:**
- **Week 3**: 2 hits (composite) vs 8 hits (hybrid) - Best ML performance
- **Week 4**: 6 hits (composite) vs 8 hits (hybrid) - Both models performed well
- **Week 16**: 2 hits (composite) vs 6 hits (hybrid) - Late season improvement
- **Week 17**: 2 hits (composite) vs 6 hits (hybrid) - Strong finish

The consistency improvement is particularly significant - the hybrid model maintained more stable performance across weeks, with fewer volatile swings between high and low-performing weeks.

## Predictive results from analyzing 16 weeks in 2024

<WeeklyPerformanceChart />

<WeeklyDataTable />

<Sticky header="Open source written in R, F#, and trained using ML.NET">
  This analysis leverages comprehensive NFL and fantasy football data sources processed through ML.NET:

  **Data Sources:**
  - **NFL Data**: `nflfastR` for player rosters and schedules; `nflreadr` for player stats, snap counts, roster data, and injury reports
  - **Fantasy Data**: FantasyPros expert consensus rankings (ECR) for players ranked 100-300, excluding QBs

  **Key Metrics Collected:**
  - Second-year player rosters from the current season
  - Snap counts with week-to-week changes (Year 1 and Year 2 data)
  - Fantasy points with performance deltas (previous week vs current)
  - Defense rankings from optional local CSV files
  - Active roster status and injury report data
  - Current week opponents for matchup analysis

  **ML Pipeline**: The combined data feeds into an ML.NET SDCA Logistic Regression model that identifies second-year fantasy breakout candidates with comprehensive metrics for predictive analysis.
  <br />
  [You can follow these directions to recreate all the data in this blog post.](https://github.com/joe307bad/fastbreak/tree/560dc7ed9f58a8ffd226eeb6aa629ecb6efba909/server/src/Fastbreak.Research.Cli/Commands/02-nfl-fantasy-breakout-predict)
</Sticky>

## Building the Hybrid Model: From Domain Expertise to Machine Learning

### Stage 1: The Composite Scoring Foundation

The composite model employs a weighted combination of six factors, mathematically expressed as:

$$
S_{total} = w_1 \cdot S_{draft} + w_2 \cdot S_{performance} + w_3 \cdot S_{age} + w_4 \cdot S_{ecr} + w_5 \cdot S_{matchup} + w_6 \cdot S_{trend}
$$

**Draft Capital Component (0-50 points)** - Weights draft position inversely, with undrafted free agents receiving maximum points based on the assumption that later selections have greater potential for performance gains.

**Performance History Component (0-30 points)** - Quantifies Year 1 production gaps, assigning higher scores to players with minimal prior fantasy output.

**Age Factor Component (0-20 points)** - Applies age-based weighting with younger players (≤22) receiving maximum scores.

**Expert Consensus Component (0-20 points)** - Incorporates FantasyPros Expert Consensus Rankings using inverse scaling to identify undervalued players.

**Matchup Analysis Component (0-30 points)** - Implements position-specific defensive matchup scoring, where running backs facing weak run defenses receive higher ratings.

**Snap Count Trends Component (0-15 points)** - Uses a two-week sliding window analysis to quantify playing time changes.

This composite score serves as the foundation for our hybrid approach - it captures established fantasy football wisdom in a quantifiable format that can be fed into machine learning algorithms.

[placeholder for bar chart showing composite score breakdown for top 5 players]

### Stage 2: Machine Learning Enhancement

The machine learning layer employs ML.NET's **SDCA Logistic Regression** (Stochastic Dual Coordinate Ascent) classifier, a sophisticated binary classification algorithm optimized for large-scale learning. This algorithm was chosen for its ability to handle high-dimensional feature spaces efficiently while providing probabilistic outputs perfect for confidence scoring.

**ML.NET Algorithm Configuration:**
- **Algorithm**: SDCA Logistic Regression with L1 and L2 regularization
- **L2 Regularization**: 0.1 (prevents overfitting to training data)
- **L1 Regularization**: 0.01 (promotes feature sparsity)
- **Maximum Iterations**: 100 (balances training time with convergence)
- **Feature Normalization**: MinMax scaling to [0,1] range

The model processes 15 key features including Sleeper Score, previous week performance, snap percentage changes, sliding window averages, and momentum indicators. The training pipeline:

```
Pipeline = Concatenate Features → Normalize MinMax → SDCA Logistic Regression
```

### The Prediction Process: From Rankings to Hits

Our evaluation methodology follows a rigorous process to ensure fair comparison between the composite scoring baseline and ML-enhanced predictions:

**Step 1: Generate Predictions**
- For each week, calculate composite (Sleeper) scores for all eligible players
- Run the ML model to generate confidence probabilities (0-100%)

**Step 2: Sort and Select Top Candidates**
- Sort all players by Sleeper score (descending) → Select top 10
- Separately, sort all players by ML confidence (descending) → Select top 10

**Step 3: Mark Hits**
- For Sleeper Top 10: Mark as "hit" if player achieved 5+ fantasy point improvement
- For ML Top 10: Mark as "hit" if player achieved 5+ fantasy point improvement

**Step 4: Calculate Confidence Scores**
The ML confidence score represents the probability of a breakout, calculated as:

$$
P(breakout) = \sigma(0.6 \cdot S_{normalized} + 0.2 \cdot I_{playtime} + 0.2 \cdot M_{momentum})
$$

Where:
- σ is the sigmoid function from the logistic regression
- S_normalized is the composite Sleeper score normalized to [0,1]
- I_playtime is a binary indicator for significant playing time
- M_momentum captures recent performance trends

This dual-sorting approach ensures we're comparing the best predictions from each method, not cherry-picking results. The ML model doesn't just re-rank Sleeper scores - it generates independent probability assessments that often identify different players entirely.

The machine learning enhancement allows the model to identify when the composite score should be weighted differently based on context. For example, it might learn that high composite scores are more predictive for running backs than wide receivers, or that snap count trends become more important late in the season.

## Week-by-Week Performance: The Hybrid Advantage

Here's where the hybrid approach really shines. When we look at how the enhanced model performed compared to using composite scoring alone, some clear patterns emerge.

**Early Season (Weeks 3-6):** The hybrid model showed immediate improvement over the baseline composite approach. Week 3 saw 8 correct predictions versus 2 from composite scoring alone, and Week 4 maintained strong performance with 8 hits versus 6 from the baseline.

**Mid-Season Volatility (Weeks 7-12):** This is where the machine learning enhancement really paid off. Week 8 demonstrated the hybrid model's superior adaptability with 7 hits compared to the baseline's 2 hits. Week 11 showed similarly strong performance with 5 ML hits versus 2 from composite scoring. Even in tough weeks like Week 10, both approaches struggled, but the hybrid model maintained advantages with 3 hits versus 2.

**Late Season Surge (Weeks 13-18):** The hybrid model's pattern recognition capabilities became increasingly valuable as the season progressed. Week 16 and Week 17 both showed 6 ML hits compared to just 2 from the baseline composite method, demonstrating strong finishing performance.

## Why the Hybrid Approach Works: Leveraging Both Strengths

### The Foundation: Composite Scoring Benefits

Using composite scoring as the primary feature brings several advantages to the machine learning model:

**Domain Knowledge Integration**: The composite score encapsulates years of fantasy football wisdom - draft capital matters, age curves exist, opportunity drives production. This prevents the ML model from having to rediscover these fundamental relationships.

**Interpretability**: Even with machine learning enhancement, the 60% weight on composite scoring means predictions remain largely explainable. You can trace why a player scored highly.

**Robust Feature Engineering**: Rather than feeding raw stats to the ML model, the composite score provides a sophisticated, pre-processed signal that captures the most important predictive factors.

### The Enhancement: Machine Learning Value-Add

The machine learning layer adds capabilities that pure composite scoring cannot achieve:

**Context-Aware Weighting**: The model learns when composite scores should be trusted more or less based on player position, team context, and game situation.

**Non-Linear Relationships**: While a composite score might treat all "high opportunity" situations equally, ML can distinguish between different types of opportunity and their likelihood of success.

**Temporal Learning**: The model adapts throughout the season, learning that certain patterns become more or less predictive as the year progresses.

## Empirical Performance Analysis

Our evaluation across 15 weeks of NFL data provides quantitative evidence for each methodology's effectiveness:

### ML.NET Model Feature Importance

The SDCA Logistic Regression model identifies which features most strongly predict breakout performances. The top 15 features used in our model, ranked by predictive importance:

1. **SleeperScore** - The composite domain expertise score (highest weight)
2. **PrevWeekFp** - Previous week's fantasy points
3. **SnapPctChange** - Week-over-week snap percentage change
4. **SlidingWindowAvgDelta** - 2-week moving average of performance changes
5. **SnapIncreaseMomentum** - Trend in snap count increases
6. **HasPositiveTrend** - Binary indicator of upward trajectory
7. **SignificantSnapJump** - Detection of 20%+ snap share increase
8. **PerformanceScore** - Historical production metrics
9. **AgeScore** - Age-based potential weighting
10. **EcrScore** - Expert consensus ranking integration
11. **MatchupScore** - Defensive matchup quality
12. **YearsExp** - Years of NFL experience
13. **AvgSnapPctY1** - First-year snap percentage average
14. **FpPerSnapY1** - First-year fantasy efficiency
15. **AvgSnapPctY2** - Second-year snap percentage average

### Model Evaluation Metrics

**ML.NET Binary Classification Metrics:**
- **AUC (Area Under ROC Curve)**: 0.826 - Excellent discrimination ability
- **Binary Accuracy**: 71.2% - Correct classification rate
- **F1 Score**: 0.604 - Balanced precision/recall performance
- **Positive Precision**: 51.6% - When predicting breakout, correct 51.6% of time
- **Positive Recall**: 72.7% - Catches 72.7% of actual breakouts

**Comparative Performance Metrics:**
- **Hybrid Model Top-10 Accuracy**: 31.9% (51 hits / 160 opportunities)
- **Baseline Composite Top-10 Accuracy**: 24.4% (39 hits / 160 opportunities)
- **Performance Improvement**: +7.5 percentage points (+31% relative improvement)
- **Overall ML Prediction Accuracy**: 34.9% (across all predictions, not just top 10)

The comparative metrics demonstrate meaningful improvement: the hybrid model correctly identified 31% more breakout performances than the baseline composite method. From a practical standpoint, this means finding significantly more of those league-winning waiver wire pickups that deliver big weeks.

## Case Study Analysis: 2024 Performance Examples

Examining specific player predictions provides insight into where the hybrid model excelled and where both methods struggled:

**Biggest ML Successes (Model Actually Predicted These):**

**Tyjae Spears (RB, Tennessee)** - Week 15: ML confidence of 51.5% correctly identified a massive 23.9-point explosion despite a low composite score of 59. This demonstrates the ML model's ability to find value where traditional metrics fall short.

**Jordan Addison (WR, Minnesota)** - Week 4: ML confidence of 70.3% correctly identified a 22.9-point breakout with low composite scoring (46). This high-confidence prediction showcases when the model is most certain.

**Quentin Johnston (WR, LAC)** - Week 9: ML confidence of 57.2% identified a 22.0-point breakout despite very low composite scoring (57). Shows the model can find value in unexpected places.

**Strong ML-Only Successes:**

**Jaxon Smith-Njigba (WR, Seattle)** - Week 11: ML confidence of 79.0% correctly predicted a 21.8-point performance while composite scoring was too low (57) to make the top 10. This was one of the highest confidence ML predictions that paid off.

**Tank Bigsby (RB, JAX)** - Week 7: ML confidence of 58.6% identified a 21.4-point breakout with composite score of 89. Shows convergence between both methods.

**Josh Downs (WR, IND)** - Week 8: ML confidence of 68.2% identified a 20.9-point performance that composite scoring missed (score: 36). Another example of the ML model detecting actionable signals that traditional analysis overlooks.

The pattern? The hybrid model excels at finding players with moderate-to-strong breakout potential that traditional methods miss, but the biggest fantasy explosions often remain unpredictable regardless of methodology.

## Understanding the Performance Differential: Data Complexity and Pattern Recognition

The superior performance of the machine learning approach stems from its capacity to process high-dimensional data and identify complex patterns. Several factors contribute to this advantage:

**High-Dimensional Feature Space**: With 60+ features per player observation, including snap count distributions, efficiency metrics, team context variables, and temporal trends, traditional linear models face significant challenges in optimal weight determination.

**Non-Linear Interaction Effects**: Consider snap count trend analysis as an illustrative example. The composite model applies uniform sliding window calculations, while ML can detect that snap count increases have differential predictive value based on:
- Player experience level (rookie vs. veteran)
- Offensive system context (high vs. low pass volume teams)
- Seasonal timing (early season adjustment vs. late season opportunity)
- Age and draft capital interactions

**Temporal Pattern Recognition**: The ML model can identify seasonal trends and adaptive patterns that static weighted models cannot capture, such as how predictive factors evolve throughout the season.

These interaction effects represent areas where complex pattern recognition provides measurable advantages over linear composite approaches, explaining the observed performance differential.

## Methodology Selection Framework

The choice between composite scoring and machine learning approaches should be guided by specific analytical objectives and operational constraints rather than a universal preference:

**Composite Scoring Applications:**
- Stakeholder explanation requirements are critical
- Limited historical data availability
- Regulatory or compliance transparency needs
- Integration of specific domain expertise is essential
- Rapid prototyping and iteration cycles

**Machine Learning Applications:**
- Predictive accuracy optimization is the primary objective
- Substantial training datasets are available
- Complex pattern recognition adds value
- Computational resources support model complexity
- User acceptance of probabilistic outputs exists

**Hybrid Methodological Approaches:**
- Different stakeholder groups with varying needs
- Composite scoring for user-facing explanations, ML for backend optimization
- Ensemble methods combining both approaches
- ML-driven feature discovery informing composite model refinement

## Future Directions in Fantasy Analytics

Our analysis illuminates several promising research directions:

**Feature Engineering Optimization**: The quality and relevance of input features may matter more than algorithmic sophistication, suggesting focus areas for data collection and preprocessing.

**Ensemble Methodologies**: Combining predictions from multiple models often outperforms individual approaches, offering a path to leverage both interpretability and accuracy.

**Adaptive Model Architecture**: Systems that can adjust parameter weights during seasons show potential for handling evolving league dynamics.

**Position-Specific Modeling**: Different player positions may benefit from specialized prediction frameworks optimized for their unique performance patterns.

## Conclusion

This analysis demonstrates that hybrid approaches can achieve meaningful improvements over traditional methods while maintaining practical advantages. By using domain expertise as the foundation and machine learning as the enhancement layer, we improved predictive accuracy by 7.5 percentage points (31% relative improvement) while keeping the model largely interpretable.

The 60/20/20 weighting (composite score/playing time/momentum) proved effective, suggesting that domain knowledge should remain the primary signal with ML providing contextual adjustments. The improvement translates to identifying 31% more actual breakout performances than composite scoring alone, meaning more league-winning waiver wire pickups across a 16-week season.

For fantasy football analytics, this hybrid approach offers a practical path forward: leverage established knowledge about what drives player performance, then use machine learning to identify when and how those patterns apply differently across contexts. The result is better predictions that fantasy players can still understand and trust.

The future of fantasy analytics likely lies in this type of thoughtful integration - not replacing human insights with algorithms, but using algorithms to enhance and contextualize human insights for better decision-making.