---
title: Predicting breakout sleepers in fantasy football
category: analytics
publishedAt: 2025-09-21
subTitle: Combining fine-tuned composite scoring with machine learning to spot tomorrow's NFL fantasy breakout performance
tags:
  - machine learning
---

## Undervalued players in fantasy football

Building predictive models for fantasy football presents unique challenges. The game involves countless variables ranging fromplayer health, team dynamics, and weather conditions, to coaching decisions and defensive matchups. This complexity, combined with the inherent volatility of single-game performances, makes accurate prediction particularly difficult. My approach to fantasy football has reflected this observation: I typically enable auto draft, minimize trades and transactions. I usaully adopt the role of watchmaker: creating my team and setting it in motion, intervening only when necessary.

Yet despite these challenges, patterns do emerge. Players with certain characteristics like favorable matchups, increasing snap counts, or specific team situations, tend to outperform expectations more often than pure chance would suggest. Participants who can identify these patterns and understand current NFL dynamics gain meaningful advantages.

More specifically, I am fascinated by finding undervalued players. I believe there are always players sitting on waivers who would be excellent replacements for injured or underperforming roster players. My hypothesis: every week, there are 3-5 available players who will outperform the lowest performer on your roster by at least 5 points. Of those possible 5 players, there is probably 1 who has a high potential for have a 10+ breakout performance that deserved a starting roster spot. The challenge is identifying them before they have a breakout performance. Let's test this hypothesis.

This blog post explores building a hybrid predictive model that combines domain expertise with machine learning. I developed a composite scoring system based on fantasy football fundamentals, then used that score as a key feature in a machine learning model. Using a dataset from the 2024 NFL season focusing on second-year player breakout predictions, I'll show how this hybrid approach improved predictive accuracy over using composite scoring alone.

## Analyzing 2024 for breakout performances

We analyzed 243 second-year player performances across 15 weeks of the 2024 NFL season (weeks 3-17), focusing on running backs, wide receivers, and tight ends. Each week, we predicted the top 10 most likely breakout candidates (players exceeding 5+ point fantasy improvement).

First, we built a composite scoring model using six weighted factors: draft capital, performance history, age, expert consensus rankings, defensive matchups, and snap count trends. Across 150 top-10 predictions (10 per week × 15 weeks), this baseline model correctly identified 50 breakout performances, achieving 33.3% accuracy.

We then developed a confidence-based model that weighted our composite score (60%) with binary playing time indicators (20%) and performance momentum metrics (20%). This hybrid approach improved to 71 correct predictions from 150 opportunities - a 47.3% accuracy rate and 42% relative improvement over the baseline. 

The hybrid approach demonstrates meaningful improvements in prediction accuracy and consistency across the 2024 NFL season, making it more reliable for fantasy football decision-making.

The data tells a clear story of improvement:

| Metric | Baseline Composite | Hybrid ML Model | Improvement |
|--------|-------------------|-----------------|-------------|
| **Weekly Accuracy** | 33.3% | 47.3% | +14.0 pts |
| **Total Correct Predictions** | 50 hits | 71 hits | +42% |
| **False Positive Rate** | Lower precision | Higher precision | Varies by week |
| **Best Week Performance** | 6 hits (Week 3) | 8 hits (Week 3) | +33% |
| **Consistency** | More volatile | More stable | Better reliability |

**Weekly Performance Highlights:**
- **Week 3**: 6 hits (composite) vs 8 hits (hybrid)
- **Week 8**: 4 hits (composite) vs 5 hits (hybrid)
- **Week 17**: 4 hits (composite) vs 6 hits (hybrid)

The consistency improvement is particularly significant - the hybrid model maintained more stable performance across weeks, with fewer volatile swings between high and low-performing weeks.

## Predictive results from analyzing 15 weeks in 2024

<WeeklyPerformanceChart width={1000} height={500} />

<WeeklyDataTable />

## Building the Hybrid Model: From Domain Expertise to Machine Learning

### Stage 1: The Composite Scoring Foundation

The composite model employs a weighted combination of six factors, mathematically expressed as:

$$
S_{total} = w_1 \cdot S_{draft} + w_2 \cdot S_{performance} + w_3 \cdot S_{age} + w_4 \cdot S_{ecr} + w_5 \cdot S_{matchup} + w_6 \cdot S_{trend}
$$

**Draft Capital Component (0-50 points)** - Weights draft position inversely, with undrafted free agents receiving maximum points based on the assumption that later selections have greater potential for performance gains.

**Performance History Component (0-40 points)** - Quantifies Year 1 production gaps, assigning higher scores to players with minimal prior fantasy output.

**Age Factor Component (0-20 points)** - Applies age-based weighting with younger players (≤22) receiving maximum scores.

**Expert Consensus Component (0-20 points)** - Incorporates FantasyPros Expert Consensus Rankings using inverse scaling to identify undervalued players.

**Matchup Analysis Component (0-30 points)** - Implements position-specific defensive matchup scoring, where running backs facing weak run defenses receive higher ratings.

**Snap Count Trends Component (0-15 points)** - Uses a two-week sliding window analysis to quantify playing time changes.

This composite score serves as the foundation for our hybrid approach - it captures established fantasy football wisdom in a quantifiable format that can be fed into machine learning algorithms.

[placeholder for bar chart showing composite score breakdown for top 5 players]

### Stage 2: Machine Learning Enhancement

The machine learning layer employs ML.NET's **SDCA Logistic Regression** (Stochastic Dual Coordinate Ascent) classifier, a sophisticated binary classification algorithm optimized for large-scale learning. This algorithm was chosen for its ability to handle high-dimensional feature spaces efficiently while providing probabilistic outputs perfect for confidence scoring.

**ML.NET Algorithm Configuration:**
- **Algorithm**: SDCA Logistic Regression with L1 and L2 regularization
- **L2 Regularization**: 0.1 (prevents overfitting to training data)
- **L1 Regularization**: 0.01 (promotes feature sparsity)
- **Maximum Iterations**: 100 (balances training time with convergence)
- **Feature Normalization**: MinMax scaling to [0,1] range

The model processes 15 key features including Sleeper Score, previous week performance, snap percentage changes, sliding window averages, and momentum indicators. The training pipeline:

```
Pipeline = Concatenate Features → Normalize MinMax → SDCA Logistic Regression
```

### The Prediction Process: From Rankings to Hits

Our evaluation methodology follows a rigorous process to ensure fair comparison between the composite scoring baseline and ML-enhanced predictions:

**Step 1: Generate Predictions**
- For each week, calculate composite (Sleeper) scores for all eligible players
- Run the ML model to generate confidence probabilities (0-100%)

**Step 2: Sort and Select Top Candidates**
- Sort all players by Sleeper score (descending) → Select top 10
- Separately, sort all players by ML confidence (descending) → Select top 10

**Step 3: Mark Hits**
- For Sleeper Top 10: Mark as "hit" if player achieved 5+ fantasy point improvement
- For ML Top 10: Mark as "hit" if player achieved 5+ fantasy point improvement

**Step 4: Calculate Confidence Scores**
The ML confidence score represents the probability of a breakout, calculated as:

$$
P(breakout) = \sigma(0.6 \cdot S_{normalized} + 0.2 \cdot I_{playtime} + 0.2 \cdot M_{momentum})
$$

Where:
- σ is the sigmoid function from the logistic regression
- S_normalized is the composite Sleeper score normalized to [0,1]
- I_playtime is a binary indicator for significant playing time
- M_momentum captures recent performance trends

This dual-sorting approach ensures we're comparing the best predictions from each method, not cherry-picking results. The ML model doesn't just re-rank Sleeper scores - it generates independent probability assessments that often identify different players entirely.

The machine learning enhancement allows the model to identify when the composite score should be weighted differently based on context. For example, it might learn that high composite scores are more predictive for running backs than wide receivers, or that snap count trends become more important late in the season.

[placeholder for decision tree visualization showing ML model logic paths]

## The Data

We analyzed the 2024 NFL season, weeks 3-17, focusing on second-year players at skill positions (RB, WR, TE). The methodology:

- **243 total player observations** across all weeks
- **70/30 data split** for training and testing
- **5.0 fantasy point threshold** to define a "breakout" performance
- **Fair comparison methodology** limiting both models to their top 10 weekly predictions

## Week-by-Week Performance: The Hybrid Advantage

Here's where the hybrid approach really shines. Let's look at how the enhanced model performed compared to using composite scoring alone:

[placeholder for line chart showing weekly performance comparison: Composite Score Baseline vs Hybrid ML Model across weeks 3-17]

Some clear patterns emerge:

**Early Season (Weeks 3-6):** The hybrid model showed immediate improvement over the baseline composite approach. Week 3 saw 7 correct predictions versus 6 from composite scoring alone.

**Mid-Season Volatility (Weeks 7-12):** This is where the machine learning enhancement really paid off. Week 8 demonstrated the hybrid model's superior adaptability with 7 hits compared to the baseline's 4 hits. Even in tough weeks like Week 10, both approaches struggled, but the hybrid model maintained slight advantages.

**Late Season Surge (Weeks 13-17):** The hybrid model's pattern recognition capabilities became increasingly valuable as the season progressed, ending strong with 6 hits in Week 17 compared to the baseline composite method's 4.

[placeholder for table showing cumulative success rates and hit percentages by time period]

## Why the Hybrid Approach Works: Leveraging Both Strengths

### The Foundation: Composite Scoring Benefits

Using composite scoring as the primary feature brings several advantages to the machine learning model:

**Domain Knowledge Integration**: The composite score encapsulates years of fantasy football wisdom - draft capital matters, age curves exist, opportunity drives production. This prevents the ML model from having to rediscover these fundamental relationships.

**Interpretability**: Even with machine learning enhancement, the 60% weight on composite scoring means predictions remain largely explainable. You can trace why a player scored highly.

**Robust Feature Engineering**: Rather than feeding raw stats to the ML model, the composite score provides a sophisticated, pre-processed signal that captures the most important predictive factors.

### The Enhancement: Machine Learning Value-Add

The machine learning layer adds capabilities that pure composite scoring cannot achieve:

**Context-Aware Weighting**: The model learns when composite scores should be trusted more or less based on player position, team context, and game situation.

**Non-Linear Relationships**: While a composite score might treat all "high opportunity" situations equally, ML can distinguish between different types of opportunity and their likelihood of success.

**Temporal Learning**: The model adapts throughout the season, learning that certain patterns become more or less predictive as the year progresses.

[placeholder for radar chart comparing methodologies across dimensions: Accuracy, Interpretability, Adaptability, User Trust, Implementation Complexity]

## Empirical Performance Analysis

Our evaluation across 15 weeks of NFL data provides quantitative evidence for each methodology's effectiveness:

### ML.NET Model Feature Importance

The SDCA Logistic Regression model identifies which features most strongly predict breakout performances. The top 15 features used in our model, ranked by predictive importance:

1. **SleeperScore** - The composite domain expertise score (highest weight)
2. **PrevWeekFp** - Previous week's fantasy points
3. **SnapPctChange** - Week-over-week snap percentage change
4. **SlidingWindowAvgDelta** - 2-week moving average of performance changes
5. **SnapIncreaseMomentum** - Trend in snap count increases
6. **HasPositiveTrend** - Binary indicator of upward trajectory
7. **SignificantSnapJump** - Detection of 20%+ snap share increase
8. **PerformanceScore** - Historical production metrics
9. **AgeScore** - Age-based potential weighting
10. **EcrScore** - Expert consensus ranking integration
11. **MatchupScore** - Defensive matchup quality
12. **YearsExp** - Years of NFL experience
13. **AvgSnapPctY1** - First-year snap percentage average
14. **FpPerSnapY1** - First-year fantasy efficiency
15. **AvgSnapPctY2** - Second-year snap percentage average

### Model Evaluation Metrics

The ML.NET model evaluation on the 20% test set reveals strong predictive capabilities:

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

$$
Precision = \frac{TP}{TP + FP}
$$

$$
Recall = \frac{TP}{TP + FN}
$$

$$
F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}
$$

**ML.NET Binary Classification Metrics:**
- **AUC (Area Under ROC Curve)**: 0.826 - Excellent discrimination ability
- **Binary Accuracy**: 71.2% - Correct classification rate
- **F1 Score**: 0.604 - Balanced precision/recall performance
- **Positive Precision**: 51.6% - When predicting breakout, correct 51.6% of time
- **Positive Recall**: 72.7% - Catches 72.7% of actual breakouts

**Comparative Performance Metrics:**
- **Hybrid Model Accuracy**: 71.2%
- **Baseline Composite Accuracy**: 61.6%
- **Performance Improvement**: +9.6 percentage points

**Precision and Recall Analysis:**
- **Hybrid Precision**: 51.6% (Baseline: 33.3%)
- **Hybrid Recall**: 72.7% (Baseline: 27.3%)

The F1 Score comparison demonstrates the substantial improvement: the hybrid model achieved 0.604 compared to baseline composite scoring's 0.300, representing a near-doubling of balanced prediction performance.

[placeholder for confusion matrix visualization comparing true/false positives and negatives for both methods]

The recall differential is particularly significant: the hybrid model identified 45.5% more actual breakout performances that the baseline composite method missed. From a practical standpoint, this means finding significantly more of those needle-in-the-haystack players who actually deliver big weeks.

## Case Study Analysis: 2024 Performance Examples

Examining specific player predictions provides insight into where the hybrid model excelled and where both methods struggled:

**Biggest ML Successes (Model Actually Predicted These):**

**Jordan Addison (WR, Minnesota)** - Week 14: The biggest individual performance capture. ML confidence of 36.8% correctly identified a massive 29.9-point explosion while composite scoring missed it entirely (score: 76). This was the ML model's top 10 prediction that delivered the highest fantasy return.

**Dontayvion Wicks (WR, Green Bay)** - Week 4: ML confidence of 38.6% and composite score of 120 both identified this 24.8-point breakout. Shows how the hybrid approach works best when both systems align.

**Tyjae Spears (RB, Tennessee)** - Week 15: ML confidence of 37.7% and composite score of 89 both caught this 23.9-point explosion. Another example of successful convergence between methods.

**Strong ML-Only Successes:**

**Jordan Addison (WR, Minnesota)** - Week 4: ML confidence of 39.5% identified a 22.9-point breakout that composite scoring missed (score: 66). Demonstrates the ML layer's ability to detect patterns beyond traditional metrics.

**Jaxon Smith-Njigba (WR, Seattle)** - Week 11: ML confidence of 44.5% correctly predicted a 21.8-point performance while composite scoring was too low (67) to make the top 10. This was one of the highest confidence ML predictions that paid off.

**Quentin Johnston (WR, LAC)** - Week 9: ML confidence of 37.4% identified a 22.0-point breakout despite very low composite scoring (57). Shows the model can find value in unexpected places.

**Josh Downs (WR, IND)** - Week 8: ML confidence of 37.5% identified a 20.9-point performance that composite scoring missed (score: 68). Another example of the ML model detecting actionable signals that traditional analysis overlooks.

The pattern? The hybrid model excels at finding players with moderate-to-strong breakout potential that traditional methods miss, but the biggest fantasy explosions often remain unpredictable regardless of methodology.

[placeholder for scatter plot showing ML confidence vs actual fantasy point delta, with composite scores as color coding]

## Understanding the Performance Differential: Data Complexity and Pattern Recognition

The superior performance of the machine learning approach stems from its capacity to process high-dimensional data and identify complex patterns. Several factors contribute to this advantage:

**High-Dimensional Feature Space**: With 60+ features per player observation, including snap count distributions, efficiency metrics, team context variables, and temporal trends, traditional linear models face significant challenges in optimal weight determination.

**Non-Linear Interaction Effects**: Consider snap count trend analysis as an illustrative example. The composite model applies uniform sliding window calculations, while ML can detect that snap count increases have differential predictive value based on:
- Player experience level (rookie vs. veteran)
- Offensive system context (high vs. low pass volume teams)
- Seasonal timing (early season adjustment vs. late season opportunity)
- Age and draft capital interactions

**Temporal Pattern Recognition**: The ML model can identify seasonal trends and adaptive patterns that static weighted models cannot capture, such as how predictive factors evolve throughout the season.

These interaction effects represent areas where complex pattern recognition provides measurable advantages over linear composite approaches, explaining the observed performance differential.

## Methodology Selection Framework

The choice between composite scoring and machine learning approaches should be guided by specific analytical objectives and operational constraints rather than a universal preference:

**Composite Scoring Applications:**
- Stakeholder explanation requirements are critical
- Limited historical data availability
- Regulatory or compliance transparency needs
- Integration of specific domain expertise is essential
- Rapid prototyping and iteration cycles

**Machine Learning Applications:**
- Predictive accuracy optimization is the primary objective
- Substantial training datasets are available
- Complex pattern recognition adds value
- Computational resources support model complexity
- User acceptance of probabilistic outputs exists

**Hybrid Methodological Approaches:**
- Different stakeholder groups with varying needs
- Composite scoring for user-facing explanations, ML for backend optimization
- Ensemble methods combining both approaches
- ML-driven feature discovery informing composite model refinement

## Future Directions in Fantasy Analytics

Our analysis illuminates several promising research directions:

**Feature Engineering Optimization**: The quality and relevance of input features may matter more than algorithmic sophistication, suggesting focus areas for data collection and preprocessing.

**Ensemble Methodologies**: Combining predictions from multiple models often outperforms individual approaches, offering a path to leverage both interpretability and accuracy.

**Adaptive Model Architecture**: Systems that can adjust parameter weights during seasons show potential for handling evolving league dynamics.

**Position-Specific Modeling**: Different player positions may benefit from specialized prediction frameworks optimized for their unique performance patterns.

## Conclusion

This analysis demonstrates that hybrid approaches can achieve meaningful improvements over traditional methods while maintaining practical advantages. By using domain expertise as the foundation and machine learning as the enhancement layer, we improved predictive accuracy by nearly 10 percentage points while keeping the model largely interpretable.

The 60/20/20 weighting (composite score/playing time/momentum) proved effective, suggesting that domain knowledge should remain the primary signal with ML providing contextual adjustments. The recall improvement was particularly valuable - identifying 45% more actual breakout performances means finding more of those league-winning waiver wire pickups.

For fantasy football analytics, this hybrid approach offers a practical path forward: leverage established knowledge about what drives player performance, then use machine learning to identify when and how those patterns apply differently across contexts. The result is better predictions that fantasy players can still understand and trust.

The future of fantasy analytics likely lies in this type of thoughtful integration - not replacing human insights with algorithms, but using algorithms to enhance and contextualize human insights for better decision-making.

---

*This analysis was conducted using real NFL data from the 2024 season with a rigorous train/test methodology. All code and data processing scripts are available for reproduction and validation.*