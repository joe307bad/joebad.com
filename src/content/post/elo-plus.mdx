---
title: Introducing Elo+ for MLB
category: analytics
publishedAt: 2025-08-25
subTitle: A revival of the fabled FiveThirtyEight Elo ratings, enhanced with ML.NET
tags:
  - machine learning
---

## What is Elo?

[Elo rating systems](https://en.wikipedia.org/wiki/Elo_rating_system) have found widespread application in competitive sports analytics. Nate Silver applied Elo ratings to [baseball in 2006](https://web.archive.org/web/20060822122806/http://baseballprospectus.com/article.php?articleid=5247) for Baseball Prospectus and later expanded the approach to [NFL predictions in 2014](https://fivethirtyeight.com/features/introducing-nfl-elo-ratings/) for FiveThirtyEight. These systems have proven highly effective for quantifying team strength and performance trends.  

## Expanding on Elo with ML

The Elo+ rating system starts <span id="1-back-to-top">[^[1]^](#1)</span> with traditional Elo ratings that rank baseball teams based on wins and losses, similar to how chess players are ranked. It then trains a machine learning model using advanced baseball statistics like pitcher WHIP (walks plus hits per inning pitched), OPS differential (on-base plus slugging percentage difference), and FIP (Fielding Independent Pitching) to predict game outcomes with a confidence score. 

When the model is highly confident in its prediction (above 60% threshold), the system applies a small adjustment to the traditional Elo rating based on whether the model's prediction was accurate. If the model correctly predicts an upset or dominant performance, teams get bonus points added to their rating, but if the model is confident yet wrong, points are subtracted as a penalty. This creates enhanced "Elo+" ratings that combine the reliability of traditional win-loss records with the predictive power of advanced baseball analytics and supervised learning algorithms.

**The Elo+ model analyzed all 2,430 MLB games from 2024 and achieved a 2.5% improvement in predictive accuracy over the baseline <span id="2-back-to-top">[^[2]^](#2)</span> using a [70/15/15 train-test split validation methodology](https://learn.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/prepare-data-ml-net#split-data-into-training--test-sets).**

## Top 10 MLB teams of 2024 ranked by Elo+

| Rank | Team | Elo+ | Standard Elo | Adjustment | ML Confidence |
|------|------|------|--------------|------------|---------------|
| 1 | Los Angeles Dodgers | 1535.1 | 1535.3 | -0.2 | 58.8% |
| 2 | Arizona Diamondbacks | 1534.9 | 1535.3 | -0.4 | 57.2% |
| 3 | Milwaukee Brewers | 1529.0 | 1529.1 | -0.1 | 57.7% |
| 4 | San Diego Padres | 1527.3 | 1527.3 | +0.1 | 59.1% |
| 5 | New York Yankees | 1525.4 | 1525.7 | -0.3 | 60.6% |
| 6 | Baltimore Orioles | 1523.8 | 1523.8 | +0.0 | 57.4% |
| 7 | Cleveland Guardians | 1523.4 | 1523.4 | -0.0 | 57.6% |
| 8 | Philadelphia Phillies | 1523.3 | 1523.2 | +0.1 | 58.3% |
| 9 | Minnesota Twins | 1522.1 | 1522.4 | -0.3 | 58.1% |
| 10 | Kansas City Royals | 1518.2 | 1517.8 | +0.4 | 61.3% |

<Sticky header="Generate the Elo+ ratings with the Fastbreak.Cli">
The code to gather the data, analyze with ML.NET and F#, and generate the Elo+ ratings is a part of an app I am building called Fastbreak that offers daily sports analytics. To see the tech in action, pull down [the code for the `Fastbreak.Cli`](https://github.com/joe307bad/fastbreak/tree/4905239625f89e5fc0ef3b3e2af95653cf1ce10d/server/src/Fastbreak.Cli) and run this command:
<br />
```
dotnet run --project src/Fastbreak.Cli -- generate-elo-plus -f test.csv -m report.md
```
</Sticky>

## Elo+ from 20,000 feet

Imagine you're trying to predict baseball games and you have two advisors: a grizzled old scout who's been watching baseball for 50 years (that's traditional Elo), and a brilliant data scientist with access to every advanced statistic known to humanity (that's our ML model). The scout is reliable and rarely gets fooled, but sometimes misses subtle patterns. The data scientist can spot incredibly complex relationships between pitcher spin rates and atmospheric pressure, but occasionally gets lost in the noise and makes overconfident predictions about meaningless correlations.

Elo+ is like having both advisors in the room, but with a smart moderator who knows when to listen to each one. Most of the time, it trusts the scout's traditional wisdom about team strength based on wins and losses. But when the data scientist is highly confident about something the scout missed - like a starting pitcher having a terrible matchup against a specific type of lineup, or advanced metrics suggesting one team is significantly better than their record shows - the system "tilts" toward the ML prediction. The key innovation is that this tilting only happens when the machine learning model is genuinely confident, not when it's just throwing statistical spaghetti at the wall.

The result is a hybrid system that combines the battle-tested reliability of traditional Elo ratings with the contextual awareness of modern machine learning. It's like upgrading from a trusty old pickup truck to the same truck with a GPS and smart cruise control - you keep what works, but add intelligence that helps you navigate tricky situations. Ready to peek under the hood and see exactly how this mathematical magic works?

## Deep dive into Elo+

When building machine learning models that can forecast future outcomes, there is always a black box of uncertainty. Certain questions arise: How was the model trained? How was it tested? What information does the model have to make informed decisions? I am going to layout the answers to these questions for Elo+.

### First, let's cover terminology: 

• **Elo Rating** - Think of Elo ratings like credit scores for sports teams. After each game, the winner gains points and the loser loses points. The amount gained/lost depends on how surprising the result was. If a strong team beats a weak team (expected result), only a few points change hands. If a weak team upsets a strong team (surprising result), many points change hands.

• **K-Factor** - Controls how much a team's rating changes after each game. Set to 4 for baseball because even good teams lose often, so we don't want ratings to swing wildly after one game.

• **Home Field Advantage** - Extra points (68) added to the home team's rating before calculating win probability. This accounts for familiar surroundings, crowd support, and not having to travel.

### Machine Learning Terms:
- **Training/Validation/Test Split** - Like studying for a test: use some games to teach the system (training), some to fine-tune it (validation), and save some games it's never seen to measure real performance (testing).

   - Three-Way Split (70%/15%/15%):
     - **Training Set (70%)**: Teach both Elo ratings and ML model what winning looks like
     - **Validation Set (15%)**: Find the best α value without cheating  
     - **Test Set (15%)**: Final, unbiased evaluation of system performance

- **Alpha Parameter** - The "volume knob" that controls how much we trust machine learning vs traditional Elo. At 0.3, we trust Elo 70% and ML 30% when making the final prediction.

- **Feature Engineering** - Choosing which baseball stats (ERA, WHIP, OPS) matter most for predicting games. Like picking the most important ingredients for a recipe.

### Baseball Statistics:
- **WHIP** - Walks plus hits per inning pitched; measures how many baserunners a pitcher allows. Lower is better for pitchers.

- **OPS** - On-base plus slugging percentage; measures how good hitters are at getting on base and hitting for power. Higher is better for batters.

- **FIP** - Fielding Independent Pitching; measures pitcher performance on things they control (strikeouts, walks, home runs), ignoring fielding. Lower is better.

And there are other advanced statistics we use to enhance predictions that won't be covered here.

### Performance Metrics:

- **Accuracy**: Simple percentage of games predicted correctly
  - **Example**: If we predict 100 games and get 58 right, accuracy = 58%
  - **Limitation**: Doesn't account for confidence - being 51% confident vs 99% confident both count the same

- **Log-Loss (Logarithmic Loss)**: Measures how confident we are in correct predictions
  - **Better**: Lower scores are better (0 is perfect, higher is worse)
  - **Example**: Predict 90% confidence and team wins: Very low penalty; Predict 90% confidence and team loses: High penalty

- **Brier Score**: Average of `(predicted_probability - actual_result)²`
  - **Range**: 0 to 1 (lower is better)
  - **Example**: Predict 70% and team wins (result=1): `(0.70 - 1.0)² = 0.09`
  - **Advantage**: Rewards both accuracy and appropriate confidence levels

**ROC AUC**: Area under receiver operating curve (discrimination ability)
- **Range**: 0.5 (random) to 1.0 (perfect)  
- **Interpretation**: Probability that model ranks a randomly chosen winning team higher than a randomly chosen losing team

## Show me the math!

There is a lot of advanced math that enables the predictive analysis behind the Elo+ system. Let's take a look step by step on how we move from traditional Elo ratings to an ML enhanced system that can better predict baseball outcomes.

###  Traditional Elo Rating Foundation

The heart of any Elo system lies in calculating how likely one team is to beat another based on their current ratings. The classic formula might look intimidating, but it's essentially asking "what are the odds?" The 400-point scale is brilliant in its simplicity - it means a team that's 400 points better has 10-to-1 odds of winning, while a 200-point advantage gives roughly 3-to-1 odds. For baseball specifically, we add 68 points to the home team's rating before calculating odds, because decades of data show home teams win about 54% of the time when teams are otherwise evenly matched. After each game, we update the losing team's rating where K=4 for MLB (a relatively small number because baseball has so much randomness that we don't want one surprising game to drastically change a team's rating).

#### Expected Win Probability: 

$$
P_{expected} = \frac{1}{1 + 10^{((R_{opponent} - R_{team}) / 400)}}
$$

This formula is the magical heart of Elo - it takes the rating difference between two teams and transforms it into a win probability that makes intuitive sense. The variables are simple: `R_team` and `R_opponent` are the current Elo ratings, while that mysterious 400 in the denominator is what makes a 400-point rating difference equal exactly 10-to-1 betting odds. Think of it like a sophisticated way to answer "if these two teams played 100 times, how many would the better team win?" **This is implemented directly in Elo+ - no ML.NET black box here, just pure mathematical transparency.**

#### Home Field Advantage: 

$$
P_{home} = \frac{1}{1 + 10^{((R_{away} - (R_{home} + HFA)) / 400)}}
$$

Here's where baseball gets its home cooking! The `HFA` (Home Field Advantage) variable is set to 68 points, which might seem random but represents decades of statistical analysis showing home teams win about 54% of games against evenly matched opponents. We literally add these bonus points to `R_home` before calculating probabilities, giving the home team a mathematical edge that reflects real-world advantages like familiar ballparks, supportive crowds, and sleeping in your own bed. **This adjustment is implemented in Elo+ before any probability calculations.**

#### Rating Update Formula: 

$$
R_{new} = R_{old} + K \times (S_{actual} - P_{expected})
$$

This is where the magic happens after each game - teams get rewarded or punished based on how surprising the result was! `S_actual` is brutally simple: 1 if you won, 0 if you lost (sorry, no participation trophies). The `K` factor of 4 acts like a volume control on how dramatically ratings change, kept low for baseball because even great teams lose 40% of their games. When a weak team upsets a strong team, `(S_actual - P_expected)` becomes a big positive number, leading to major rating swings. **Every single rating update uses this exact formula, implemented in Elo+ rather than outsourced to ML.NET.**

### Core Elo+ Tilting Models

Here's where Elo+ gets interesting - we have three different mathematical approaches for blending traditional Elo predictions with machine learning insights. The Linear Combination approach is like having two weather forecasters where you trust one more than the other - if α=0.3, you're giving the ML model a 30% voice and traditional Elo a 70% voice in the final prediction. The Weighted Average model gives you even more control by letting you set independent trust levels for each system. The most sophisticated approach is Confidence-Weighted which only "tilts" toward the ML prediction when the machine learning model is highly confident - if the ML model is wishy-washy about a prediction, the system gracefully falls back to pure Elo.

**Important note**: While all three tilting models are implemented in the system, the Elo+ model uses only **one approach at a time** based on configuration. The default setup uses Linear Combination with α=0.3 (trusting traditional Elo 70% and ML 30%), but you could easily switch to Weighted Average or Confidence-Weighted approaches by changing the system configuration. This design allows for experimentation to determine which tilting model performs best for baseball predictions, though the current implementation defaults to the simplest and most interpretable Linear Combination approach.

#### Linear Combination Model:

$$ 
P_{final} = (1 - \alpha) \times P_{elo} + \alpha \times P_{ml}
$$

This is the simplest way to blend two forecasters - imagine you have a wise old baseball scout (`P_elo`) and a young data scientist (`P_ml`) making predictions. The `α` (alpha) parameter is like a volume mixer: when α = 0.3, you're giving 70% weight to the traditional scout and 30% to the analytics nerd. It's mathematically elegant and surprisingly effective - most of the time, the best predictions come from listening to both voices rather than picking just one. **This linear combination is implemented directly in Elo+, where you can see exactly how the two probabilities get weighted and combined.**

#### Weighted Average Model: 

$$
P_{final} = \frac{w_{elo} \times P_{elo} + w_{ml} \times P_{ml}}{w_{elo} + w_{ml}}
$$

This approach gives you independent volume controls for each system - think of it like a DJ mixing two tracks where `w_elo` and `w_ml` are separate sliders. Unlike the linear model, you can set `w_elo = 7` and `w_ml = 3` to get the same 70/30 split, but you could also use `w_elo = 2` and `w_ml = 1` for finer control. The denominator ensures everything adds up to proper probabilities, making this perfect when you want asymmetric trust levels in your prediction systems. **Like the linear model, this weighted average calculation is implemented in Elo+ - no external libraries needed for this math.**

#### Confidence-Weighted Model: 

$$
P_{final} = P_{elo} + \alpha \times C_{ml} \times (P_{ml} - P_{elo})
$$

This is the most sophisticated tilting approach - it only nudges away from traditional Elo when the ML model is confident about being different! `C_ml` represents how sure the machine learning model is about its prediction (0 = totally unsure, 1 = absolutely certain). When the ML model is wishy-washy, `C_ml` approaches zero and the formula gracefully falls back to pure Elo. But when the ML model screams "I'm 95% sure the underdog wins!", that high confidence gets multiplied by the difference `(P_ml - P_elo)`, creating a strong tilt toward the ML prediction. **This confidence-weighted logic is implemented in Elo+, giving us full control over how confidence affects the final prediction blend.**

### Hyperparameter Optimization and Performance Evaluation

The system doesn't just guess at the best α value - it systematically tests different values where the loss function L measures how wrong our predictions were on games the model had never seen before. We use Log-Loss as our primary metric because it heavily penalizes overconfident wrong predictions (like being 90% sure a team will win when they actually lose). The Brier Score is essentially the average squared error between our predicted probabilities and the actual outcomes - think of it as measuring both "were we right?" and "were we appropriately confident?" To ensure our enhanced system is truly better, we compare against a baseline accuracy which represents the performance of always picking whichever outcome (home wins or away wins) happened more often in our dataset.

#### Alpha Optimization: 

$$
\alpha^* = \arg\min_\alpha L(\text{validation\_set}, \alpha)
$$

This intimidating formula is actually asking a simple question: "What's the best volume setting for our ML/Elo mixer?" The `α*` (alpha-star) represents the optimal tilting parameter we're searching for, while `L` is our loss function that measures how wrong our predictions were. By testing different α values on games the model has never seen (the validation set), we find the sweet spot where our hybrid predictions are most accurate - no guessing, just cold hard math! **The grid search optimization that finds this optimal α is implemented in Elo+, systematically testing values from 0.0 to 1.0 to find the best blend.**

#### Log-Loss (Cross-Entropy): 

$$
\text{LogLoss} = -\frac{1}{N} \times \sum[y_i \times \log(p_i) + (1-y_i) \times \log(1-p_i)]
$$

Log-Loss is like a lie detector for probability predictions - it severely punishes overconfident wrong guesses! Here `y_i` is what actually happened (1 for home win, 0 for away win), `p_i` is your predicted probability, and `N` is the total number of games. The logarithm amplifies penalties exponentially: predict 90% confidence and be wrong, you get hammered way harder than if you'd predicted 60% and been wrong. This forces our Elo+ system to be appropriately humble about its predictions. **This log-loss calculation is implemented in Elo+, where we manually compute the logarithmic penalty for each prediction.**

#### Brier Score: 

$$
\text{BrierScore} = \frac{1}{N} \times \sum(p_i - y_i)^2
$$

The Brier Score is wonderfully simple - it's just the average squared difference between what you predicted (`p_i`) and what actually happened (`y_i`). If you predict 70% chance of a home win and they win, you get `(0.7 - 1.0)² = 0.09` points. If you predict 70% and they lose, you get `(0.7 - 0.0)² = 0.49` points. Lower is better, and unlike accuracy, this rewards both being right AND being appropriately confident about your predictions. **The Brier score calculation is implemented in Elo+ - we compute the squared errors ourselves rather than relying on external libraries.**

#### Accuracy: 

$$
\text{Accuracy} = \frac{\text{CorrectPredictions}}{\text{TotalPredictions}}
$$

Good old accuracy - the most honest metric of them all! Simply count up how many games you got right, divide by total games, and you get a percentage that anyone can understand. The beauty of this metric for Elo+ is that it directly answers "how often does this system pick the winner?" But it has a blind spot - predicting with 51% confidence vs 99% confidence both count the same if you're right. **This straightforward calculation is implemented in Elo+ - we simply count correct predictions and divide by total, no fancy algorithms needed.**

#### Precision: 

$$
\text{Precision} = \frac{\text{TruePositives}}{\text{TruePositives} + \text{FalsePositives}}
$$

Precision asks "when you predict a home team will win, how often are you right?" True Positives are correct home win predictions, while False Positives are when you wrongly predicted a home win. For Elo+ baseball predictions, high precision means you're not crying wolf - when you say the home team will win, they usually do. This is crucial for betting applications where false alarms can be expensive! **The precision calculation is implemented in Elo+, where we manually track and compute the true positive and false positive counts.**

#### Recall (Sensitivity): 

$$
\text{Recall} = \frac{\text{TruePositives}}{\text{TruePositives} + \text{FalseNegatives}}
$$

Recall flips the question: "Of all the games where the home team actually won, how many did you predict correctly?" False Negatives are the painful misses - games where the home team won but you predicted they'd lose. High recall means you're good at spotting home team victories before they happen. In baseball prediction, you want both precision AND recall to be high, which is why we also calculate the F1 score. **Like precision, recall is implemented in Elo+, tracking true positives and false negatives to measure our "hit rate" on actual home wins.**

#### F1-Score: 

$$
F1 = \frac{2 \times (\text{Precision} \times \text{Recall})}{\text{Precision} + \text{Recall}}
$$

The F1 score is the peacemaker between precision and recall - it's their harmonic mean, which sounds fancy but just means it penalizes extreme imbalances. If you have 90% precision but 10% recall (or vice versa), your F1 score will be terrible. This forces the Elo+ system to be good at both accurately predicting home wins (precision) AND not missing actual home wins (recall), creating a more balanced and reliable predictor. **The F1 calculation is implemented in Elo+, using the standard harmonic mean formula to balance precision and recall.**

#### ROC AUC: 

$$
\text{AUC} = \sum[(\text{FPR}_{i+1} - \text{FPR}_i) \times (\text{TPR}_i + \text{TPR}_{i+1}) / 2]
$$

ROC AUC (Area Under the Curve) measures how well your system can rank teams - it's asking "if I pick a random winning team and a random losing team, what's the probability your model ranks the winner higher?" The formula uses trapezoidal integration over False Positive Rate (FPR) and True Positive Rate (TPR) to calculate discrimination ability. A perfect score of 1.0 means your model never ranks a losing team higher than a winning team, while 0.5 is pure random guessing. **ROC AUC calculation is implemented in Elo+ using trapezoidal approximation - no external statistics libraries, just pure mathematical integration.**

#### Calibration Error: 

$$
\text{CalError} = \sum[w_i \times |f_i - o_i|]
$$

Calibration error checks if your probabilities actually mean what they claim to mean! We bin predictions by probability ranges (0-10%, 10-20%, etc.), then compare the average predicted probability (`f_i`) in each bin to the actual win rate (`o_i`) in that bin. The `w_i` weights ensure bins with more predictions have more influence. If you predict 70% confidence in 100 games and exactly 70 of them win, you're perfectly calibrated - your probabilities are honest! **The calibration analysis is implemented in Elo+, where we manually bin predictions and compute weighted calibration errors.**

### Statistical Validation Framework and Theoretical Foundations

The entire system rests on rigorous statistical validation using a 70%/15%/15% chronological split where we train on older games, validate hyperparameters on middle games, and test final performance on the most recent games - this prevents any "time travel" cheating where future information leaks into past predictions. The theoretical foundation assumes that team performance follows a normal distribution where σ = 200, meaning most games will be close to what we'd expect, but there's enough randomness for upsets to happen regularly. Our ML Enhancement Objective is simply trying to make our predicted probabilities as close as possible to the actual game outcomes, leveraging contextual information like pitcher matchups and recent team form that traditional Elo systems ignore entirely.

#### Train/Validation/Test Split: 
- 70% / 15% / 15%

#### Baseline Accuracy: 

$$
\text{Baseline} = \text{Accuracy}_{\text{VanillaElo}}
$$

This is our reality check - the gold standard that any enhanced system must beat! Our baseline is the accuracy of traditional vanilla Elo ratings on the same test set, representing decades of proven sports prediction methodology. If vanilla Elo correctly predicts 58% of games, that's our baseline to beat. For Elo+, beating this baseline by 2.5% means our machine learning enhancements provide genuine improvement over the time-tested Elo foundation, not just some naive guessing strategy. **This baseline is implemented in Elo+ by calculating vanilla Elo predictions on the test set and measuring their accuracy.**


#### Cross-Validation: 

$$
\text{CV\_Score} = \frac{1}{k} \times \sum_{i=1}^k \text{Score}_i
$$

Cross-validation is like getting multiple second opinions before making a big decision! We split our data into `k` folds (typically 5 or 10), train on k-1 folds, test on the remaining fold, and repeat this process `k` times. The `CV_Score` is simply the average performance across all folds. This gives us a more robust estimate of how well our Elo+ system will perform on truly unseen data, helping us avoid the trap of hyperparameters that only work well on one specific dataset split. **The k-fold cross-validation logic is implemented in Elo+, where we manually manage the temporal splitting and averaging across folds.**

#### Elo Performance Distribution: 

$$
\text{Performance} \sim N(\text{Rating}, \sigma^2) \text{ where } \sigma = 200
$$

This is the theoretical foundation that makes Elo work - it assumes that a team's performance in any given game follows a normal (bell curve) distribution centered on their true rating. The `σ = 200` means that about 68% of the time, a team will perform within 200 points of their rating, and 95% of the time within 400 points. This mathematical assumption is what allows us to convert rating differences into meaningful win probabilities - without it, the whole system would just be arbitrary numbers! **This is a theoretical assumption rather than implemented code - it's the mathematical foundation that justifies our 400-point scale and probability calculations throughout the Elo system.**

#### ML Enhancement Objective: 

$$
\text{Minimize } E[(P_{\text{predicted}} - P_{\text{actual}})^2]
$$

This is the North Star that guides our entire Elo+ enhancement - minimize the expected squared difference between our predicted probabilities and actual game outcomes! `E[...]` represents the expected value (average over all games), while `P_predicted` is what our hybrid system forecasts and `P_actual` is brutal reality (1.0 for actual wins, 0.0 for actual losses). By incorporating machine learning features like pitcher WHIP and team OPS, we're trying to reduce this prediction error beyond what traditional Elo alone can achieve. **This objective function drives everything we do in Elo+, though it's more of a guiding principle than a single line of code - it's realized through our implementations of hyperparameter optimization, model training (via ML.NET), and the confidence-based tilting mechanisms that blend Elo and ML predictions.**

<div className="border-t-2 border-dotted border-(--color-primary-500) py-2 mt-4"></div>

<span id="1">1.</span> The Elo+ rating system uses [baseballr](https://cran.r-project.org/package=baseballr) for data collection and [ML.NET](https://dotnet.microsoft.com/en-us/apps/machinelearning-ai/ml-dotnet) for advanced machine learning capabilities. <a href="#1-back-to-top">^</a>

<span id="2">2.</span> The baseline for Elo+ is traditional vanilla Elo ratings - the time-tested system that has been predicting sports outcomes for decades. If the Elo+ model beats vanilla Elo by 2.5%, it means our machine learning enhancements provide 2.5 percentage points higher accuracy than standard Elo predictions. This demonstrates that incorporating advanced baseball analytics like pitcher WHIP, team OPS, and contextual features genuinely improves upon the proven Elo foundation. <a href="#2-back-to-top">^</a>